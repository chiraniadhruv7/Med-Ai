

# ğŸ¥ Explainable Multimodal Clinical AI Assistant

**CNN + MedGemma Fusion for Chest X-ray Analysis with Grad-CAM Explainability**

[![Python 3.11+](https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-009688?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.30+-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io)
[![Apple Silicon](https://img.shields.io/badge/Apple_Silicon-Optimized-000000?style=for-the-badge&logo=apple&logoColor=white)](https://developer.apple.com/metal/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![Open In Colab](https://img.shields.io/badge/Open_in_Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/chiraniadhruv7/Med-Ai/blob/main/Med_Ai.ipynb)

<br>

> âš ï¸ **For research and educational purposes only. Not clinically validated or intended for diagnostic use.**

<br>

<img src="assets/ui_screenshot.png" alt="Clinical AI Assistant UI" width="90%">

</div>

---

## ğŸ“– Overview

A production-quality, **modular medical AI system** that fuses **DenseNet121 CNN** pathology detection with **MedGemma 4B** multimodal LLM reasoning to analyze chest X-rays. The system provides:

- ğŸ”¬ **14-class pathology detection** from chest X-rays using DenseNet121
- ğŸ§  **Multimodal clinical reasoning** via MedGemma (local MLX inference â€” fully private)
- ğŸ”¥ **Grad-CAM visual explainability** showing where the AI looks
- ğŸ“Š **Confidence-calibrated fusion** merging CNN + LLM outputs with contradiction detection
- âš¡ **Apple Silicon optimized** â€” MPS acceleration, no CUDA required
- ğŸ¨ **Premium Streamlit UI** with dark medical theme

---

## ğŸ—ï¸ System Architecture

```mermaid
flowchart TB
    subgraph Input["ğŸ“¥ Input"]
        IMG[ğŸ©» Chest X-ray]
        SYM[ğŸ“ Symptom Text]
    end

    subgraph Pipeline["âš™ï¸ Analysis Pipeline"]
        CNN["ğŸ”¬ DenseNet121 CNN<br><i>14 pathology probabilities</i>"]
        LLM["ğŸ§  MedGemma 4B<br><i>Clinical reasoning + differentials</i>"]
        FUS["ğŸ”€ Fusion Module<br><i>Agreement boost + contradiction detection</i>"]
        UNC["ğŸ“Š Uncertainty Estimation<br><i>Entropy-based confidence calibration</i>"]
        RSK["âš ï¸ Risk Stratification<br><i>Rule-based: Low / Moderate / High</i>"]
        CAM["ğŸ”¥ Grad-CAM<br><i>Visual attention heatmap</i>"]
    end

    subgraph Output["ğŸ“¤ Output"]
        JSON["ğŸ“‹ Structured JSON"]
        UI["ğŸ–¥ï¸ Streamlit Dashboard"]
    end

    IMG --> CNN
    IMG --> LLM
    SYM --> LLM
    CNN --> FUS
    LLM --> FUS
    FUS --> UNC
    UNC --> RSK
    CNN --> CAM
    RSK --> JSON
    CAM --> JSON
    JSON --> UI

    style Input fill:#1e3a5f,stroke:#38bdf8,color:#e2e8f0
    style Pipeline fill:#0f172a,stroke:#475569,color:#e2e8f0
    style Output fill:#064e3b,stroke:#34d399,color:#e2e8f0
```

### Pipeline Modules

| Step | Module | Description | Output |
|:----:|--------|-------------|--------|
| 1 | `cnn_model.py` | DenseNet121 pretrained on ImageNet, modified for 14 CXR pathologies | `dict[str, float]` â€” probability per pathology |
| 2 | `multimodal_reasoning.py` | MedGemma 4B via MLX (local) or Gemini API (fallback) | Clinical reasoning, differential diagnosis |
| 3 | `fusion.py` | Merges CNN + LLM results, detects model contradictions | Adjusted findings with agreement/conflict flags |
| 4 | `uncertainty.py` | Entropy-based CNN confidence + LLM token confidence | Per-source and fused confidence scores |
| 5 | `risk_module.py` | Rule-based stratification (thresholds on pathology probs) | `Low` / `Moderate` / `High` |
| 6 | `explainability.py` | Grad-CAM on DenseNet121 final conv layer | Base64-encoded heatmap overlay |

---

## âš¡ Quick Start

### Prerequisites

| Requirement | Version | Notes |
|-------------|---------|-------|
| macOS | 13+ (Ventura) | Apple Silicon (M1/M2/M3/M4) recommended |
| Python | 3.11+ | 3.11 recommended; 3.13 compatible |
| Xcode CLT | Latest | `xcode-select --install` |
| Disk Space | ~12 GB | For MedGemma model weights |
| RAM | 16 GB+ | 8 GB minimum (model uses ~5 GB) |

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/Med-Ai.git
cd Med-Ai

# 2. Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate

# 3. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Edit .env if needed (defaults work for local inference)

# 5. Download MedGemma model (~10.2 GB, one-time download)
python -c "
from huggingface_hub import snapshot_download
snapshot_download('mlx-community/medgemma-4b-it-8bit')
print('Download complete!')
"

# 6. Verify setup
python -c "import torch; print(f'PyTorch: {torch.__version__}, MPS: {torch.backends.mps.is_available()}')"
```

> **ğŸ’¡ Tip:** The MedGemma download requires a Hugging Face account. Run `huggingface-cli login` first if prompted.

### Running the Application

You need **two terminal windows** â€” one for the backend and one for the frontend:

```bash
# Terminal 1: Start FastAPI backend
source venv/bin/activate
uvicorn main:app --host 127.0.0.1 --port 8000
```

Wait for `[MedGemma] Local model loaded successfully.` then:

```bash
# Terminal 2: Start Streamlit frontend
source venv/bin/activate
streamlit run app.py
```

Open **http://localhost:8501** in your browser.

---

## ğŸ““ Try It in Google Colab

Don't have an Apple Silicon Mac or prefer a cloud-based setup? You can run the entire analysis pipeline directly in **Google Colab** â€” no local installation required.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chiraniadhruv7/Med-Ai/blob/main/Med_Ai.ipynb)

### Colab Prerequisites

| Requirement | Details |
|-------------|---------|
| **Google Account** | Required to access Google Colab |
| **GPU Runtime** | Select **Runtime â†’ Change runtime type â†’ T4 GPU** (free tier works) |
| **Hugging Face Token** | Required to download MedGemma weights â€” [get one here](https://huggingface.co/settings/tokens) |
| **MedGemma License** | Accept the model license at [google/medgemma-4b-it](https://huggingface.co/google/medgemma-4b-it) before running |
| **Kaggle API Key** | Required to download the NIH Chest X-ray dataset â€” [create one here](https://www.kaggle.com/settings) |
| **NIH Labels CSV** | Upload `Data_Entry_2017.csv` when prompted (from the [NIH Chest X-ray dataset](https://www.kaggle.com/datasets/nih-chest-xrays/data)) |

### What the Notebook Covers

- ğŸš€ **One-click setup** â€” installs all dependencies (PyTorch, Transformers, etc.) inside the Colab runtime
- ğŸ“¦ **Dataset download** â€” pulls real chest X-ray images from Kaggle via API
- ğŸ”¬ **CNN fine-tuning** â€” trains DenseNet121 on 14 CXR pathologies with mixed-precision training
- ğŸ“Š **Full evaluation** â€” AUC-ROC curves, confusion matrices, and optimal threshold selection
- ğŸ”¥ **Grad-CAM visualization** â€” see heatmap overlays highlighting model attention regions
- ğŸ§  **MedGemma reasoning** â€” multimodal clinical reasoning powered by MedGemma 4B
- ğŸ”€ **Fusion & risk assessment** â€” CNN + LLM fusion with contradiction detection and risk stratification

> **ğŸ’¡ Tip:** The free T4 GPU runtime is sufficient. Training takes ~2 min/epoch and full inference runs in seconds.

---

## ğŸ–¥ï¸ Usage

### Through the Web UI

1. **Upload** a chest X-ray image (PNG, JPG, JPEG)
2. **Enter** patient symptoms (e.g., *"persistent cough for 5 days, fever 101Â°F, shortness of breath"*)
3. Click **ğŸ”¬ Analyze**
4. View results:
   - **Risk Badge** â€” Color-coded severity (ğŸŸ¢ Low / ğŸŸ¡ Moderate / ğŸ”´ High)
   - **Confidence Bars** â€” CNN, LLM, and Fused confidence scores
   - **Visual Findings** â€” 14 pathology probabilities with progress bars
   - **Clinical Reasoning** â€” MedGemma's natural language analysis
   - **Differential Diagnosis** â€” Ranked list of possible conditions
   - **Model Contradictions** â€” Where CNN and LLM disagree
   - **Grad-CAM Heatmap** â€” Toggle to see where the AI focused

### Through the API

```bash
# Health check
curl http://127.0.0.1:8000/health

# Analyze an X-ray
curl -X POST http://127.0.0.1:8000/analyze \
  -F "image=@path/to/xray.png" \
  -F "symptoms=persistent cough and fever"
```

### API Response Format

```json
{
  "visual_findings": {
    "Pneumonia": 0.77,
    "Effusion": 0.78,
    "Atelectasis": 0.48,
    "Cardiomegaly": 0.29,
    "...": "..."
  },
  "clinical_reasoning": "Based on the radiographic findings and reported symptoms...",
  "differential_diagnosis": ["Pneumonia", "Bronchitis", "Viral URI"],
  "confidence_scores": {
    "cnn": 0.03,
    "llm": 0.70,
    "fused": 0.31
  },
  "risk_level": "Moderate",
  "risk_factors": ["High probability: Effusion (0.79)", "High probability: Pneumonia (0.77)"],
  "contradictions": [
    {
      "pathology": "Atelectasis",
      "cnn_probability": 0.48,
      "note": "CNN detected but LLM did not mention"
    }
  ],
  "heatmap_available": true,
  "heatmap_base64": "<base64-encoded PNG>"
}
```

---

## ğŸ“ Project Structure

```
med-ai2/
â”œâ”€â”€ main.py                         # FastAPI server â€” orchestrates the full pipeline
â”œâ”€â”€ app.py                          # Streamlit UI â€” premium dark medical theme
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cnn_model.py                # DenseNet121 â€” 14-class CXR pathology detection
â”‚   â”œâ”€â”€ multimodal_reasoning.py     # MedGemma â€” local MLX / API / dummy inference
â”‚   â”œâ”€â”€ fusion.py                   # CNN + LLM fusion with contradiction detection
â”‚   â”œâ”€â”€ risk_module.py              # Rule-based risk stratification
â”‚   â”œâ”€â”€ uncertainty.py              # Entropy-based confidence estimation
â”‚   â””â”€â”€ explainability.py           # Grad-CAM heatmap generation
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ metrics.py                  # AUC-ROC, confusion matrix utilities
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py            # End-to-end pipeline tests (6 tests)
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ sample_xray.png             # Sample test image
â”‚   â”œâ”€â”€ ui_screenshot.png           # UI screenshot
â”‚   â””â”€â”€ demo_screenshot.png         # Demo with results
â”‚
â”œâ”€â”€ Med_Ai.ipynb                    # ğŸ““ Colab notebook â€” run the full pipeline in the cloud
â”œâ”€â”€ requirements.txt                # Apple Silicon compatible dependencies
â”œâ”€â”€ .env.example                    # Environment variable template
â”œâ”€â”€ .env                            # Local environment config (git-ignored)
â””â”€â”€ README.md
```

---

## ğŸ§  Technical Deep Dive

### MedGemma Inference

The system uses **MedGemma 4B IT** (`mlx-community/medgemma-4b-it-8bit`) quantized to 8-bit for efficient Apple Silicon inference via the [MLX](https://github.com/ml-explore/mlx) framework.

| Property | Value |
|----------|-------|
| Model | `mlx-community/medgemma-4b-it-8bit` |
| Parameters | 4 billion (8-bit quantized) |
| Size on disk | ~10.2 GB |
| Framework | MLX + mlx-vlm |
| Inference device | Apple Silicon (Metal) |
| Fallback | Google Gemini API â†’ Dummy mode |

**Inference modes** (configured via `MEDGEMMA_MODE` in `.env`):

| Mode | How it works |
|------|-------------|
| `local` | Loads model weights from HuggingFace cache, runs on Apple Silicon via MLX |
| `api` | Sends requests to Google Gemini API (requires `GEMINI_API_KEY`) |
| `dummy` | Returns templated responses (no model required â€” for testing) |

### CNN Pathology Detection

**DenseNet121** pretrained on ImageNet with a modified classifier head for 14 standard CXR pathologies:

```
Atelectasis Â· Cardiomegaly Â· Effusion Â· Infiltration Â· Mass Â· Nodule
Pneumonia Â· Pneumothorax Â· Consolidation Â· Edema Â· Emphysema
Fibrosis Â· Pleural Thickening Â· Hernia
```

> **Note:** The model uses ImageNet pretrained weights, not CheXpert/MIMIC-trained weights. Probabilities reflect learned visual features but are not clinically calibrated.

### Fusion & Contradiction Detection

The fusion module performs intelligent merging:

1. **Agreement boost** â€” When both CNN and LLM identify the same condition, confidence increases
2. **Contradiction detection** â€” When CNN flags a pathology that LLM doesn't mention (or vice versa), the system flags it and reduces confidence
3. **Weighted combination** â€” CNN (visual) and LLM (clinical reasoning) scores are blended based on per-source confidence

### Grad-CAM Explainability

Gradient-weighted Class Activation Mapping on DenseNet121's final convolutional layer:

- Registers forward/backward hooks to capture activations and gradients
- Computes channel-wise gradient weights
- Generates a heatmap overlay on the original X-ray
- Returns as base64-encoded PNG for frontend display

**Technical fix applied:** DenseNet121 uses `inplace=True` ReLU operations that conflict with PyTorch's autograd backward hooks. The implementation deep-copies the model and patches all ReLU operations to `inplace=False` before computing Grad-CAM.

---

## ğŸ§ª Testing

```bash
# Run all pipeline tests
python tests/test_pipeline.py

# Expected output:
# ============================================================
#   Explainable Clinical AI â€” Pipeline Tests
# ============================================================
# [TEST] CNN prediction...        âœ“ 14 pathologies predicted
# [TEST] MedGemma dummy mode...   âœ“ Reasoning generated
# [TEST] Fusion module...         âœ“ Fused findings computed
# [TEST] Uncertainty estimation... âœ“ Confidence scores calibrated
# [TEST] Risk stratification...   âœ“ High/Low risk correctly assigned
# [TEST] Grad-CAM explainability. âœ“ Heatmap generated
# ============================================================
#   âœ… ALL TESTS PASSED
# ============================================================
```

### API Testing

```bash
# Start the backend first, then:

# Health check
curl http://127.0.0.1:8000/health
# â†’ {"status": "healthy", "cnn_ready": true}

# Full analysis
curl -X POST http://127.0.0.1:8000/analyze \
  -F "image=@assets/sample_xray.png" \
  -F "symptoms=persistent cough and fever"
```

---

## âš™ï¸ Configuration

### Environment Variables (`.env`)

| Variable | Default | Description |
|----------|---------|-------------|
| `MEDGEMMA_MODE` | `local` | Inference mode: `local`, `api`, or `dummy` |
| `GEMINI_API_KEY` | *(empty)* | Required only if `MEDGEMMA_MODE=api` |
| `BACKEND_HOST` | `127.0.0.1` | FastAPI server host |
| `BACKEND_PORT` | `8000` | FastAPI server port |

### Apple Silicon (MPS) Notes

- PyTorch **automatically uses MPS** (Metal Performance Shaders) on Apple Silicon
- Falls back to CPU if MPS is unavailable
- No CUDA or NVIDIA GPU required
- MedGemma runs via MLX, which is native to Apple Silicon

---

## ğŸ”§ Troubleshooting

| Issue | Solution |
|-------|----------|
| `SSL: CERTIFICATE_VERIFY_FAILED` on model download | Run `/Applications/Python 3.x/Install Certificates.command` or `pip install certifi` |
| MedGemma falls back to dummy mode | Run the download command from the installation steps |
| `MPS not available` | Update macOS to 13+ and PyTorch to 2.1+ |
| Streamlit can't connect to backend | Ensure FastAPI is running on port 8000 before starting Streamlit |
| Out of memory during inference | Close other applications; MedGemma needs ~5 GB RAM |
| Slow first inference | First run compiles MLX kernels â€” subsequent runs are faster |

---

## ğŸ“š Dependencies

| Category | Packages |
|----------|----------|
| **Core ML** | `torch>=2.1.0`, `torchvision>=0.16.0`, `numpy`, `scikit-learn` |
| **Backend** | `fastapi>=0.109.0`, `uvicorn[standard]`, `python-multipart` |
| **Frontend** | `streamlit>=1.30.0`, `httpx` |
| **MedGemma (Local)** | `mlx>=0.25.0`, `mlx-vlm>=0.2.0` |
| **MedGemma (API)** | `google-generativeai>=0.5.0` |
| **Visualization** | `matplotlib>=3.8.0`, `opencv-python-headless>=4.9.0` |
| **Utilities** | `python-dotenv>=1.0.0`, `Pillow>=10.0.0` |

---




---

## ğŸ“„ License

This project is for **research and educational purposes only**. It is not intended for clinical diagnosis or medical decision-making.

---

<div align="center">

**Built with â¤ï¸ for medical AI research**

*Always consult a qualified healthcare professional for medical advice.*

</div>

